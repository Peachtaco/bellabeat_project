{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "In this case study, I will be analyzing a public dataset using SQL for Bellabeat, a high-tech manufacturer of health-focused products for women. I'll be looking at smart device data to gain insight on how consumers use their smart devices and provide recommendations to the Bellabeat marketing strategy.\n",
    "\n",
    "# Background\n",
    "\n",
    "Bellabeat was founded in 2013 by Urška Sršen and Sando Mur with the goal of developing beautifully designed technology that would inform and inspire women. The technology would collect data on activity, sleep, stress, and reproductive health to empower women with knowledge about their own health.\n",
    "\n",
    "Bellabeat products are available at a number of online retailers in addition to their website. The company has invested in traditional advertising media such as radio, billboards, print, and television, but focuses on digital marketing extensively. They have ads on Youtube and Google, and are active on multiple social media platforms including Facebook, Instagram, and Twitter.\n",
    "\n",
    "**Bellabeat Products**\n",
    "- Bellabeat app provides users with health data related to their activity, sleep, stress, menstrual cycle, and mindfulness habits. This data can help users better understand their current habits and make healthy decisions. The Bellabeat app connects to their line of smart wellness products.\n",
    "- Leaf: A wellness tracker that can be worn as a bracelet, necklace, or clip and connects to the Bellabeat app to track activity, sleep, and stress.\n",
    "- Time: A wellness watch with smart technology and connects to the Bellabeat app to track user activity, sleep, and stress.\n",
    "- Spring: A water bottle with smart technology and connects to the Bellabeat app to track daily water intake.\n",
    "- Bellabeat membership: A subscription-based membership program for users to have 24/7 access to fully personalized guidance on nutrition, activity, sleep, health and beauty, and mindfulness based on their lifestyle and goals.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Business Task\n",
    "\n",
    "Analyze smart device data to gain insight on how consumers use smart devices and answer the following questions:\n",
    "\n",
    "- What are some trends in smart device usage?\n",
    "- How could these trends apply to Bellabeat customers?\n",
    "- How could these trends help influence Bellabeat marketing strategy?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prepare Data\n",
    "\n",
    "The dataset was obtained from [Kaggle](https://www.kaggle.com/datasets/arashnic/fitbit) and contains FitBit tracker data from 30 users including minute, hourly, and daily level output data for activity intensity, steps, calories, sleep, and heart rate. The dataset contained 18 CSV files, each file containing a table varying in number of columns and content. Using Pandas and a Python loop, each file was read to a dataframe then written to a PostgreSQL database as a new table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "#Environment setup\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "%load_ext sql\n",
    "\n",
    "#Connect to SQL database\n",
    "%sql postgresql://postgres:postgres@mydataspace.cgaor8iekxp8.us-east-1.rds.amazonaws.com:5432/testbellabase\n",
    "engine = sqlalchemy.create_engine('postgresql://postgres:postgres@mydataspace.cgaor8iekxp8.us-east-1.rds.amazonaws.com:5432/testbellabase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "940"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/dailyactivity.csv')\n",
    "    #df.columns = df.columns.str.lower() #convert column names to lower case\n",
    "df.to_sql('dailyactivity', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m tablename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(names)\n\u001b[0;32m      8\u001b[0m tablename, ext \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(tablename)\n\u001b[1;32m----> 9\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(names)\n\u001b[0;32m     10\u001b[0m \u001b[39m#df.columns = df.columns.str.lower() #convert column names to lower case\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df\u001b[39m.\u001b[39mto_sql(tablename, engine, if_exists\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Andy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Andy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Andy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Andy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\Andy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Andy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\Andy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Andy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Andy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Andy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Andy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1158\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Andy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[39m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[0;32m   1425\u001b[0m     \u001b[39m#  here too.\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m     \u001b[39m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m     \u001b[39m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[0;32m   1430\u001b[0m     )\n\u001b[1;32m-> 1433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m   1434\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m \u001b[39m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1436\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(arr_or_dtype, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Read multiple CSV files and load into SQL database\n",
    "import glob \n",
    "import os\n",
    "file_names = glob.glob('data/*.csv')\n",
    "\n",
    "for names in file_names:\n",
    "    tablename = os.path.basename(names)\n",
    "    tablename, ext = os.path.splitext(tablename)\n",
    "    df = pd.read_csv(names)\n",
    "    #df.columns = df.columns.str.lower() #convert column names to lower case\n",
    "    df.to_sql(tablename, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://postgres:***@mydataspace.cgaor8iekxp8.us-east-1.rds.amazonaws.com:5432/bellabeat\n",
      "7 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>tablename</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>heartrate_seconds</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>dailyactivity</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>hourlydata</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>minutedata</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>minutesleep</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>sleepday</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>weightloginfo</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[('heartrate_seconds',),\n",
       " ('dailyactivity',),\n",
       " ('hourlydata',),\n",
       " ('minutedata',),\n",
       " ('minutesleep',),\n",
       " ('sleepday',),\n",
       " ('weightloginfo',)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "-- Verify tables in database\n",
    "SELECT tablename\n",
    "FROM pg_catalog.pg_tables\n",
    "WHERE schemaname = 'public';"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data\n",
    "\n",
    "With so many tables to work with, I wanted to look at all the data in one place to see if I can simplify the dataset by removing any tables with redundant information or consolidate tables with similar information.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src='images\\databasediagram.png' width=90%>\n",
    "</p>\n",
    "\n",
    "- The `dailycalories`, `dailyintensities`, and `dailysteps` tables where dropped as the data already existed in the `dailyactivity` table. \n",
    "- A new table with hourly data was created by joining the `hourlycalories`, `hourlyintensities`, and `hourlysteps` tables.\n",
    "- Tables containing minute data were dropped as the data is too granular to extract any meaningful insights. \n",
    "\n",
    "- There were 3 duplicate rows in the `sleepday` table that were removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Drop unnecessary tables\n",
    "DROP TABLE IF EXISTS dailycalories, dailyintensities, dailysteps, minutecalorieswide, minuteintensitieswide, minutestepswide;\n",
    "\n",
    "-- Verify tables have been dropped\n",
    "SELECT tablename\n",
    "FROM pg_catalog.pg_tables\n",
    "WHERE schemaname = 'public';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Join hourly tables and create new table\n",
    "CREATE TABLE hourlydata AS(\n",
    "SELECT c.id,\n",
    "       c.activityhour,\n",
    "       c.calories,\n",
    "       i.totalintensity,\n",
    "       i.averageintensity,\n",
    "       s.steptotal\n",
    "FROM hourlycalories AS c\n",
    "FULL OUTER JOIN hourlyintensities AS i\n",
    "    ON c.id = i.id\n",
    "    AND c.activityhour = i.activityhour\n",
    "FULL OUTER JOIN hourlysteps AS s\n",
    "    ON i.id = s.id\n",
    "    AND i.activityhour = s.activityhour\n",
    ");\n",
    "\n",
    "-- Drop joined tables\n",
    "DROP TABLE IF EXISTS hourlycalories, hourlyintensities, hourlysteps;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-DELETE THIS MAYBE\n",
    "\n",
    "%%sql\n",
    "--Create new table from joined minute tables\n",
    "CREATE TABLE minutedata AS(\n",
    "SELECT c.id,\n",
    "       c.activityminute,\n",
    "       c.calories,\n",
    "       i.intensity,\n",
    "       s.steps,\n",
    "       m.mets\n",
    "FROM minutecaloriesnarrow AS c\n",
    "FULL OUTER JOIN minuteintensitiesnarrow AS i\n",
    "    ON c.id = i.id\n",
    "    AND c.activityminute = i.activityminute\n",
    "FULL OUTER JOIN minutestepsnarrow AS s\n",
    "    ON i.id = s.id\n",
    "    AND i.activityminute = s.activityminute\n",
    "FULL OUTER JOIN minutemetsnarrow AS m\n",
    "    ON s.id = m.id\n",
    "    AND s.activityminute = m.activityminute\n",
    ");\n",
    "\n",
    "--Drop joined tables\n",
    "DROP TABLE IF EXISTS minutecaloriesnarrow, minuteintensitiesnarrow, minutestepsnarrow, minutemetsnarrow;\n",
    "\n",
    "-- Verify tables have been dropped\n",
    "SELECT tablename\n",
    "FROM pg_catalog.pg_tables\n",
    "WHERE schemaname = 'public';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "set() -> new empty set object\n",
      "set(iterable) -> new set object\n",
      "\n",
      "Build an unordered collection of unique elements.\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     LazySet, LazySet, LazySet, OrderedSet\n"
     ]
    }
   ],
   "source": [
    "how many unique devices are in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "--Delete duplicate rows\n",
    "DELETE\n",
    "FROM sleepday\n",
    "WHERE ctid IN (SELECT ctid\n",
    "               FROM (SELECT ctid,\n",
    "                            ROW_NUMBER() OVER (PARTITION BY id, sleepday) AS rn\n",
    "                     FROM sleepday) AS temptable\n",
    "               WHERE rn > 1);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "### Analyze Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb7b32d548d82b6a7ff40a390ea4827db719455eaed557f2375b621b2072d11f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
